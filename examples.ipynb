{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DynamoDB Data Source for PySpark — Examples\n",
        "\n",
        "This notebook demonstrates all supported operations:\n",
        "- **Batch Write** — write a DataFrame to a DynamoDB table\n",
        "- **Batch Write with Delete Flag** — conditionally delete items during a write\n",
        "- **Batch Read** — read an entire table into a DataFrame\n",
        "- **Batch Read with Parallel Scan** — use multiple segments for faster reads\n",
        "- **Batch Read with Schema Projection** — read only specific columns\n",
        "- **Streaming Write** — write a streaming DataFrame to DynamoDB\n",
        "- **Service Credentials** — authenticate via Databricks Unity Catalog service credentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Install the wheel (if running outside Databricks):\n",
        "```bash\n",
        "pip install dist/dynamodb_data_source-0.1.0-py3-none-any.whl\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from dynamodb_data_source import DynamoDbDataSource\n",
        "\n",
        "spark = SparkSession.builder.appName(\"dynamodb-examples\").getOrCreate()\n",
        "spark.dataSource.register(DynamoDbDataSource)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Common options — update these for your environment\n",
        "common_options = {\n",
        "    \"table_name\": \"<your-table-name>\",\n",
        "    \"aws_region\": \"us-west-2\",\n",
        "    \"aws_access_key_id\": \"<your-access-key-id>\",\n",
        "    \"aws_secret_access_key\": \"<your-secret-access-key>\",\n",
        "    \"aws_session_token\": \"<your-session-token>\",  # optional, for temporary credentials\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Batch Write"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = [\n",
        "    (\"id-001\", \"Alice\", 30, 100),\n",
        "    (\"id-002\", \"Bob\", 25, 200),\n",
        "    (\"id-003\", \"Charlie\", 35, 150),\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\", \"score\"])\n",
        "\n",
        "# create_table will create the table if it doesn't exist, using \"id\" as the hash key.\n",
        "# If the table already exists, this is a no-op.\n",
        "df.write.format(\"dynamodb\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .options(**common_options) \\\n",
        "    .option(\"create_table\", \"true\") \\\n",
        "    .option(\"hash_key\", \"id\") \\\n",
        "    .save()\n",
        "\n",
        "print(f\"Wrote {df.count()} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch Write with Repartitioning (for large DataFrames)\n",
        "\n",
        "Repartitioning distributes the write across multiple Spark tasks, each with its own\n",
        "`batch_writer` connection to DynamoDB. This is the recommended approach for large writes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "large_data = [(f\"id-{i:05d}\", f\"User_{i}\", i % 100, i * 10) for i in range(10_000)]\n",
        "df_large = spark.createDataFrame(large_data, [\"id\", \"name\", \"age\", \"score\"])\n",
        "\n",
        "df_large.repartition(4).write.format(\"dynamodb\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .options(**common_options) \\\n",
        "    .save()\n",
        "\n",
        "print(f\"Wrote {df_large.count()} rows using 4 parallel partitions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Batch Write with Delete Flag\n",
        "\n",
        "Rows where the delete flag column matches the specified value will be **deleted** from\n",
        "DynamoDB. All other rows are written (upserted) as normal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_with_deletes = [\n",
        "    (\"id-001\", \"Alice\", 30, 100, False),   # upsert\n",
        "    (\"id-002\", \"Bob\", 25, 200, True),      # delete\n",
        "    (\"id-004\", \"Diana\", 28, 300, False),   # insert\n",
        "]\n",
        "df_del = spark.createDataFrame(data_with_deletes, [\"id\", \"name\", \"age\", \"score\", \"is_deleted\"])\n",
        "\n",
        "df_del.write.format(\"dynamodb\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .options(**common_options) \\\n",
        "    .option(\"delete_flag_column\", \"is_deleted\") \\\n",
        "    .option(\"delete_flag_value\", \"true\") \\\n",
        "    .save()\n",
        "\n",
        "print(\"Write with delete flag complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Batch Write with TTL\n",
        "\n",
        "DynamoDB TTL is just a regular numeric attribute containing a Unix epoch timestamp.\n",
        "Enable TTL on the table via the AWS console or API, then include the TTL column in your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "expire_in_30_days = int(time.time()) + 86400 * 30\n",
        "\n",
        "data_with_ttl = [\n",
        "    (\"id-010\", \"Temporary User\", expire_in_30_days),\n",
        "    (\"id-011\", \"Another Temp\", expire_in_30_days + 3600),\n",
        "]\n",
        "df_ttl = spark.createDataFrame(data_with_ttl, [\"id\", \"name\", \"expire_at\"])\n",
        "\n",
        "df_ttl.write.format(\"dynamodb\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .options(**common_options) \\\n",
        "    .save()\n",
        "\n",
        "print(\"Write with TTL column complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Batch Read\n",
        "\n",
        "Reads the entire table. Schema is automatically derived by sampling items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_read = spark.read.format(\"dynamodb\") \\\n",
        "    .options(**common_options) \\\n",
        "    .load()\n",
        "\n",
        "print(f\"Read {df_read.count()} rows, columns: {df_read.columns}\")\n",
        "\n",
        "display(df_read)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch Read with Parallel Scan\n",
        "\n",
        "Set `total_segments` to distribute the scan across multiple Spark tasks.\n",
        "AWS recommends 1 segment per 2 GB of table data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_parallel = spark.read.format(\"dynamodb\") \\\n",
        "    .options(**common_options) \\\n",
        "    .option(\"total_segments\", \"4\") \\\n",
        "    .load()\n",
        "\n",
        "print(f\"Read {df_parallel.count()} rows using 4 parallel segments\")\n",
        "\n",
        "display(df_parallel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch Read with Consistent Reads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_consistent = spark.read.format(\"dynamodb\") \\\n",
        "    .options(**common_options) \\\n",
        "    .option(\"consistent_read\", \"true\") \\\n",
        "    .load()\n",
        "\n",
        "print(f\"Read {df_consistent.count()} rows with strongly consistent reads\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch Read with Schema Projection\n",
        "\n",
        "Provide an explicit schema to read only specific columns. This uses DynamoDB's\n",
        "`ProjectionExpression` to fetch only the requested attributes, reducing data transfer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "projection_schema = StructType([\n",
        "    StructField(\"id\", StringType()),\n",
        "    StructField(\"name\", StringType()),\n",
        "])\n",
        "\n",
        "df_projected = spark.read.format(\"dynamodb\") \\\n",
        "    .schema(projection_schema) \\\n",
        "    .options(**common_options) \\\n",
        "    .load()\n",
        "\n",
        "print(f\"Projected columns: {df_projected.columns}\")\n",
        "\n",
        "display(df_projected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Streaming Write\n",
        "\n",
        "Write a streaming DataFrame to DynamoDB. This example uses the built-in `rate` source\n",
        "to generate test data. Replace with your actual streaming source (Kafka, Kinesis, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "stream_df = (\n",
        "    spark.readStream\n",
        "    .format(\"rate\")\n",
        "    .option(\"rowsPerSecond\", 5)\n",
        "    .option(\"numPartitions\", 1)\n",
        "    .load()\n",
        "    .withColumn(\"id\", F.concat(F.lit(\"stream-\"), F.col(\"value\").cast(\"string\")))\n",
        "    .withColumn(\"name\", F.lit(\"streamed_item\"))\n",
        "    .select(\"id\", \"name\")\n",
        ")\n",
        "\n",
        "query = (\n",
        "    stream_df.writeStream\n",
        "    .format(\"dynamodb\")\n",
        "    .outputMode(\"append\")\n",
        "    .options(**common_options)\n",
        "    .option(\"checkpointLocation\", \"/tmp/dynamodb_stream_checkpoint\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "print(f\"Streaming query started: {query.id}\")\n",
        "print(\"Run query.stop() to stop the stream\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop the streaming query when done\n",
        "# query.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Using Databricks Unity Catalog Service Credentials\n",
        "\n",
        "On Databricks (DBR 15.4+), you can authenticate using a Unity Catalog service credential\n",
        "instead of explicit AWS keys. Set the `credential_name` option to the name of your\n",
        "service credential."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resolve AWS credentials from a Unity Catalog service credential\n",
        "from databricks.sdk.runtime import dbutils\n",
        "\n",
        "uc_service_credential_name = \"<your-service-credential-name>\"\n",
        "provider = dbutils.credentials.getServiceCredentialsProvider(uc_service_credential_name)\n",
        "credentials = provider.get_credentials().get_frozen_credentials()\n",
        "\n",
        "uc_options = {\n",
        "    \"table_name\": \"<your-table-name>\",\n",
        "    \"aws_region\": \"us-west-2\",\n",
        "    \"aws_access_key_id\": credentials.access_key,\n",
        "    \"aws_secret_access_key\": credentials.secret_key,\n",
        "    \"aws_session_token\": credentials.token,\n",
        "    \"credential_name\": uc_service_credential_name\n",
        "}\n",
        "\n",
        "# Batch write with service credentials\n",
        "df.write.format(\"dynamodb\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .options(**uc_options) \\\n",
        "    .save()\n",
        "\n",
        "# Batch read with service credentials\n",
        "df = spark.read.format(\"dynamodb\") \\\n",
        "    .options(**uc_options) \\\n",
        "    .load()\n",
        "\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Configuration Reference\n",
        "\n",
        "| Option | Required | Default | Description |\n",
        "|--------|----------|---------|-------------|\n",
        "| `table_name` | Yes | — | DynamoDB table name |\n",
        "| `aws_region` | Yes | — | AWS region (e.g. `us-east-1`) |\n",
        "| `aws_access_key_id` | No | — | AWS access key (uses default credentials if not set) |\n",
        "| `aws_secret_access_key` | No | — | AWS secret key |\n",
        "| `aws_session_token` | No | — | AWS session token for temporary credentials |\n",
        "| `endpoint_url` | No | — | Custom endpoint (e.g. `http://localhost:8000` for DynamoDB Local) |\n",
        "| `credential_name` | No | — | Databricks Unity Catalog service credential name |\n",
        "| `delete_flag_column` | No | — | Column indicating deletion (write only) |\n",
        "| `delete_flag_value` | No | — | Value that triggers deletion (write only) |\n",
        "| `total_segments` | No | `1` | Number of parallel scan segments (read only) |\n",
        "| `consistent_read` | No | `false` | Use strongly consistent reads (read only) |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
